<div align="right">

![Version](https://img.shields.io/badge/Version-0.0.1-blue?style=flat) 
![License](https://img.shields.io/badge/License-MIT-blue?style=flat) 
![Python](https://img.shields.io/badge/Language-Python-blue?style=flat) 

</div>

# Desaf√≠o LATAM Airlines
Este proyecto resuelve el desaf√≠o de ingenier√≠a de datos enfocado en procesamiento eficiente de grandes vol√∫menes de tweets (~398MB), optimizando tanto memoria como tiempo de ejecuci√≥n para el an√°lisis de datos de Twitter.

**Repositorio GitHub:** [latam-data-challenge](https://github.com/landerox/latam-data-challenge)

### Descripci√≥n del Challenge

El desaf√≠o consiste en procesar un dataset de tweets para resolver tres problemas principales:

1. **Top 10 fechas con m√°s tweets** y el usuario m√°s activo por d√≠a
2. **Top 10 emojis m√°s usados** con su conteo respectivo  
3. **Top 10 usuarios m√°s influyentes** basado en menciones (@)

Cada problema debe implementarse con **dos enfoques**:
- **Optimizaci√≥n de tiempo**: M√°xima velocidad de procesamiento
- **Optimizaci√≥n de memoria**: Uso m√≠nimo de recursos de memoria

### Estructura del Repositorio

```
.
‚îú‚îÄ‚îÄ Dockerfile                      # Configuraci√≥n para containerizaci√≥n
‚îú‚îÄ‚îÄ LICENSE                         # Licencia del proyecto
‚îú‚îÄ‚îÄ README.md                       # Documentaci√≥n principal
‚îú‚îÄ‚îÄ exploration/                    # Scripts de exploraci√≥n de datos
‚îÇ   ‚îú‚îÄ‚îÄ explore_tweets_q1.py
‚îÇ   ‚îú‚îÄ‚îÄ explore_tweets_q2.py
‚îÇ   ‚îî‚îÄ‚îÄ explore_tweets_q3.py
‚îú‚îÄ‚îÄ notebooks/                      # An√°lisis detallado en Jupyter
‚îÇ   ‚îî‚îÄ‚îÄ challenge_analysis.ipynb
‚îú‚îÄ‚îÄ poetry.lock                     # Lock file de Poetry
‚îú‚îÄ‚îÄ pyproject.toml                  # Configuraci√≥n del proyecto
‚îú‚îÄ‚îÄ requirements.txt                # Dependencies para pip
‚îú‚îÄ‚îÄ src/                           # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ config.json                # Configuraci√≥n GCP y dataset
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # CLI principal con argparse
‚îÇ   ‚îú‚îÄ‚îÄ q1_memory.py               # Pregunta 1 - optimizado para memoria
‚îÇ   ‚îú‚îÄ‚îÄ q1_time.py                 # Pregunta 1 - optimizado para tiempo
‚îÇ   ‚îú‚îÄ‚îÄ q2_memory.py               # Pregunta 2 - optimizado para memoria
‚îÇ   ‚îú‚îÄ‚îÄ q2_time.py                 # Pregunta 2 - optimizado para tiempo
‚îÇ   ‚îú‚îÄ‚îÄ q3_memory.py               # Pregunta 3 - optimizado para memoria
‚îÇ   ‚îú‚îÄ‚îÄ q3_time.py                 # Pregunta 3 - optimizado para tiempo
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Utilidades: GCS, BigQuery, config
‚îî‚îÄ‚îÄ terraform/                     # Infraestructura como C√≥digo
    ‚îú‚îÄ‚îÄ Makefile                   # Comandos de automatizaci√≥n
    ‚îú‚îÄ‚îÄ backend.tf                 # Configuraci√≥n del backend
    ‚îú‚îÄ‚îÄ main.tf                    # Recursos principales
    ‚îú‚îÄ‚îÄ modules/                   # M√≥dulos reutilizables
    ‚îÇ   ‚îú‚îÄ‚îÄ bigquery/             # M√≥dulo BigQuery
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset/          # Creaci√≥n de dataset
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tables/           # Creaci√≥n de tablas
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tables_schemas/  # Esquemas JSON
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ q1_results.json
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ q2_results.json
    ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ q3_results.json
    ‚îÇ   ‚îú‚îÄ‚îÄ bucket/               # M√≥dulo Cloud Storage
    ‚îÇ   ‚îî‚îÄ‚îÄ serviceAccount/       # M√≥dulo Service Account
    ‚îú‚îÄ‚îÄ outputs.tf                # Outputs de Terraform
    ‚îú‚îÄ‚îÄ variables.tf              # Variables de configuraci√≥n
    ‚îú‚îÄ‚îÄ terraform.tfvars.example  # Ejemplo de configuraci√≥n
    ‚îî‚îÄ‚îÄ versions.tf               # Versiones de providers
```

### Flujo de Trabajo Completo

### 1. Setup Inicial (Una sola vez)
```bash
# Clonar repositorio
git clone https://github.com/landerox/latam-data-challenge
cd latam-data-challenge

# Configurar infraestructura
cd terraform
make set-project PROJECT_ID=tu-project-id
make enable-apis
make create-sa
make add-sa-roles
make create-sa-key
make create-tf-bucket

# Configurar variables
cp terraform.tfvars.example terraform.tfvars
# Editar terraform.tfvars

# Aplicar infraestructura
make init
make apply
```

### 2. Configuraci√≥n CI/CD (Una sola vez)
```bash
# En GitHub Repository Settings:
# 1. Agregar Secrets: SA_TERRAFORM_KEY, SA_DEPLOYMENT_KEY
# 2. Agregar Variables: PROJECT_ID, REGION, etc.
# 3. Push a main/develop activa el pipeline autom√°tico
```

### 3. Ejecuci√≥n de An√°lisis
```bash
# Autom√°tico via CI/CD (push a main/develop)
git push origin main

# Manual via Cloud Run Job
gcloud run jobs execute latam-data-challenge-job \
  --project=latam-data-challenge \
  --region=us-east1 \
  --args="src/main.py,--question,q1,--method,time,--save_bq" \
  --wait

# Local para desarrollo
poetry run python src/main.py --question all --method memory
```

### Configuraci√≥n de Infraestructura (IaC)

#### Setup Inicial

1. **Configuraci√≥n de Terraform:**
   ```bash
   cd terraform
   cp terraform.tfvars.example terraform.tfvars
   # Edita terraform.tfvars con tus configuraciones
   ```

### Setup Inicial con Makefile

1. **Configuraci√≥n de proyecto:**
   ```bash
   cd terraform
   make set-project PROJECT_ID=tu-project-id
   make enable-apis
   ```

2. **Creaci√≥n de Service Account:**
   ```bash
   make create-sa
   make add-sa-roles
   make create-sa-key  # ‚ö†Ô∏è NO commitear este archivo
   ```

3. **Bucket de Terraform:**
   ```bash
   make create-tf-bucket
   ```

4. **Configuraci√≥n de variables:**
   ```bash
   cp terraform.tfvars.example terraform.tfvars
   # Edita terraform.tfvars con tus valores
   ```

#### Recursos Creados

La infraestructura automatizada crea:

- **Bucket de Terraform State:** `gs://latam-data-challenge-terraform-state`
- **Bucket de datos:** `gs://latam-data-challenge-data`
- **Dataset BigQuery:** `challenge_data` con 3 tablas:
  - `q1_results`: Fechas con m√°s tweets y usuario m√°s activo
  - `q2_results`: Ranking de emojis m√°s usados
  - `q3_results`: Usuarios m√°s mencionados
- **Cuenta de servicio:** `sa-deployment` con roles:
  - BigQuery Data Editor
  - Storage Object Admin
  - Cloud Run Developer

#### Esquemas de BigQuery

**Tabla q1_results:**
```json
{
  "tweet_date": "DATE",
  "top_user": "STRING", 
  "method": "STRING",
  "ingested_at": "TIMESTAMP"
}
```

**Tabla q2_results:**
```json
{
  "emoji": "STRING",
  "count": "INTEGER",
  "method": "STRING", 
  "ingested_at": "TIMESTAMP"
}
```

**Tabla q3_results:**
```json
{
  "username": "STRING",
  "mention_count": "INTEGER",
  "method": "STRING",
  "ingested_at": "TIMESTAMP"
}
```

#### CI/CD Autom√°tico

El pipeline ejecuta autom√°ticamente en cada push a `main` o `develop`:

**Job 1: Terraform Workflow**
- `terraform plan` - Planificaci√≥n de cambios
- `terraform apply` - Aplicaci√≥n de infraestructura (solo main/develop)

**Job 2: Build & Deploy**
- Build de imagen Docker
- Push a Artifact Registry
- Deploy como Cloud Run Job
- Ejecuci√≥n autom√°tica del an√°lisis

### Variables de Entorno Requeridas

**GitHub Repository Variables:**
```bash
PROJECT_ID=latam-data-challenge
REGION=us-east1
ZONE=us-east1-b
ARTIFACT_REPO=latam-data-challenge-repo
IMAGE_NAME=latam-data-challenge
CLOUD_RUN_JOB_NAME=latam-data-challenge-job
ENVIRONMENT=dev
REPOSITORY_ID=latam-data-challenge
```

**GitHub Repository Secrets:**
```bash
SA_TERRAFORM_KEY      # Service Account key para Terraform
SA_DEPLOYMENT_KEY     # Service Account key para deployment
```

### Despliegue en Cloud Run Job

El proyecto se despliega autom√°ticamente como un **Cloud Run Job** mediante GitHub Actions. Una vez desplegado, el job puede ser ejecutado manualmente con diferentes par√°metros.

#### Configuraci√≥n del Cloud Run Job

```yaml
# Configuraci√≥n autom√°tica via CI/CD
Resource: 2 CPU, 4Gi memoria
Timeout: 3600s (1 hora)
Service Account: sa-deployment@latam-data-challenge.iam.gserviceaccount.com
Execution Environment: gen2
Max Retries: 1
```

#### Ejecuci√≥n Manual desde Cloud Shell

Una vez desplegado, puedes ejecutar el job con diferentes configuraciones:

```bash
# An√°lisis completo (todas las preguntas, m√©todo tiempo)
gcloud run jobs execute latam-data-challenge-job \
  --project=latam-data-challenge \
  --region=us-east1 \
  --wait

# Solo pregunta 1 optimizada para memoria
gcloud run jobs execute latam-data-challenge-job \
  --project=latam-data-challenge \
  --region=us-east1 \
  --overrides='
  {
    "spec": {
      "template": {
        "spec": {
          "template": {
            "spec": {
              "containers": [
                {
                  "args": ["src/main.py", "--question", "q1", "--method", "memory", "--save_bq"]
                }
              ]
            }
          }
        }
      }
    }
  }' \
  --wait

# Top 5 emojis con m√©todo tiempo
gcloud run jobs execute latam-data-challenge-job \
  --project=latam-data-challenge \
  --region=us-east1 \
  --overrides='
  {
    "spec": {
      "template": {
        "spec": {
          "template": {
            "spec": {
              "containers": [
                {
                  "args": ["src/main.py", "--question", "q2", "--method", "time", "--top_n", "5", "--save_bq"]
                }
              ]
            }
          }
        }
      }
    }
  }' \
  --wait
```

#### Comandos Simplificados

Para facilitar la ejecuci√≥n, puedes usar estos comandos m√°s simples:

```bash
# Variables de entorno
export PROJECT_ID="latam-data-challenge"
export REGION="us-east1"
export JOB_NAME="latam-data-challenge-job"

# Ejecutar an√°lisis espec√≠fico
gcloud run jobs execute $JOB_NAME \
  --project=$PROJECT_ID \
  --region=$REGION \
  --args="src/main.py,--question,q3,--method,memory,--top_n,15,--save_bq" \
  --wait
```

### Configuraci√≥n Local para Desarrollo

#### Ejecuci√≥n Local

```bash
# Instalar dependencias
poetry install

# Configurar variable de entorno para testing local
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"

# Ejecutar localmente
poetry run python src/main.py --question q1 --method time --top_n 5

# Con archivo local (sin Cloud Storage)
poetry run python src/main.py --question all --method memory --top_n 10
```

#### Docker Local

```bash
# Build imagen localmente
docker build -t latam-data-challenge .

# Ejecutar contenedor
docker run --rm \
  -v /path/to/service-account.json:/app/sa.json \
  -e GOOGLE_APPLICATION_CREDENTIALS=/app/sa.json \
  latam-data-challenge \
  python src/main.py --question q2 --method time --save_bq
```

#### Ejemplos de Uso

```bash
# Ejecutar pregunta 1 optimizada para tiempo
poetry run python src/main.py --question q1 --method time

# Ejecutar todas las preguntas optimizadas para memoria y guardar en BigQuery
poetry run python src/main.py --question all --method memory --save_bq

# Ejecutar pregunta 2 con top 5 resultados
poetry run python src/main.py --question q2 --method time --top_n 5

# An√°lisis completo: todas las preguntas con ambos m√©todos
poetry run python src/main.py --question all --method time --save_bq
poetry run python src/main.py --question all --method memory --save_bq
```

#### Estructura de Salida

**Q1 - Top fechas con m√°s tweets:**
```python
[(datetime.date(2021, 2, 12), "narendramodi"), 
 (datetime.date(2021, 2, 13), "SatyagrahF"), ...]
```

**Q2 - Top emojis m√°s usados:**
```python  
[("üôè", 5049), ("üòÇ", 3072), ("üî•", 2972), ...]
```

**Q3 - Usuarios m√°s mencionados:**
```python
[("narendramodi", 2265), ("Kisanektamorcha", 1840), ...]
```

### Estrategias de Optimizaci√≥n

#### Time Optimization
- **Pandas DataFrame**: Carga completa del dataset en memoria para operaciones vectorizadas
- **Agregaciones eficientes**: Uso de `groupby()` y `value_counts()` de pandas
- **Procesamiento por lotes**: Manejo de todo el dataset de una vez
- **Indexaci√≥n autom√°tica**: Aprovecha los √≠ndices internos de pandas para b√∫squedas r√°pidas

#### Memory Optimization  
- **Streaming line-by-line**: Procesamiento secuencial del archivo JSON Lines
- **Collections.Counter**: Estructura de datos eficiente para conteos incrementales
- **Liberaci√≥n inmediata**: Variables se procesan y liberan l√≠nea por l√≠nea
- **Sin carga completa**: El dataset nunca se carga completamente en memoria

#### Implementaciones Espec√≠ficas

**Q1 - Top fechas y usuarios m√°s activos:**
- **Time**: Pandas groupby con agregaciones m√∫ltiples por fecha y usuario
- **Memory**: Dict anidado `{date: {username: count}}` con procesamiento l√≠nea por l√≠nea

**Q2 - Top emojis m√°s usados:**
- **Time**: Pandas Series con `str.findall()` y `explode()` para extraer emojis
- **Memory**: Regex pattern con Counter incremental por cada tweet

**Q3 - Usuarios m√°s mencionados:**
- **Time**: Lista plana de menciones con pandas `value_counts()`
- **Memory**: Counter directo sobre el campo `mentionedUsers` de cada tweet

### Buenas Pr√°cticas Aplicadas

- ‚úÖ **Arquitectura modular**: Separaci√≥n clara entre optimizaciones de tiempo y memoria
- ‚úÖ **Manejo robusto de errores**: Validaci√≥n de JSON y campos requeridos
- ‚úÖ **Configuraci√≥n centralizada**: `config.json` para par√°metros de infraestructura
- ‚úÖ **Documentaci√≥n completa**: Docstrings detallados en todas las funciones
- ‚úÖ **GitFlow workflow**: Ramas de desarrollo separadas del main
- ‚úÖ **Infrastructure as Code**: Terraform con m√≥dulos reutilizables
- ‚úÖ **CI/CD automatizado**: GitHub Actions para deploy continuo
- ‚úÖ **Logging estructurado**: Seguimiento detallado de ejecuci√≥n
- ‚úÖ **Timezone awareness**: Manejo correcto de husos horarios
- ‚úÖ **Escalabilidad**: Dise√±o preparado para datasets m√°s grandes
- ‚úÖ **Testing de integraci√≥n**: Validaci√≥n con datos reales
- ‚úÖ **Cleanup autom√°tico**: Eliminaci√≥n de datos previos por partici√≥n

### Supuestos del Proyecto

- **Formato de datos**: JSON Lines con estructura de Twitter API v1
- **Campos requeridos**: 
  - `date`: Timestamp ISO 8601 del tweet
  - `user.username`: Usuario autor del tweet
  - `content`: Contenido textual para an√°lisis de emojis
  - `mentionedUsers`: Array de usuarios mencionados
- **Encoding**: UTF-8 para caracteres especiales y emojis
- **Timezone**: America/Santiago para timestamps de ingesti√≥n
- **Manejo de errores**: L√≠neas malformadas se omiten silenciosamente
- **Permisos GCP**: Service Account con acceso a Storage y BigQuery
- **Recursos disponibles**: Suficiente memoria/CPU para dataset de ~398MB

### Librer√≠as Clave

```python
# Procesamiento de datos
pandas>=1.5.0              # An√°lisis de datos y agregaciones
numpy>=1.21.0              # Operaciones num√©ricas

# Google Cloud Platform
google-cloud-storage>=2.7.0   # Descarga desde Cloud Storage
google-cloud-bigquery>=3.4.0  # Inserci√≥n de resultados

# An√°lisis y profiling
memory-profiler>=0.60.0    # An√°lisis de memoria (opcional)

# Utilidades del sistema
pathlib                    # Manejo de rutas (stdlib)
tempfile                   # Archivos temporales (stdlib)
collections.Counter        # Conteos eficientes (stdlib)
json                       # Parsing JSON Lines (stdlib)
re                         # Regex para emojis (stdlib)
datetime                   # Manejo de fechas (stdlib)
zoneinfo                   # Timezone Santiago (stdlib)
```

#### Configuraci√≥n del Proyecto

El archivo `src/config.json` contiene:
```json
{
  "BUCKET": "latam-data-challenge-data",
  "DATASET_ID": "challenge_data", 
  "FILENAME": "farmers-protest-tweets-2021-2-4.json",
  "PROJECT_ID": "latam-data-challenge"
}
```

### Monitoreo y Logs

#### Cloud Logging

Todos los logs se almacenan en Cloud Logging con etiquetas:
```bash
# Ver logs del Cloud Run Job
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=latam-data-challenge-job" \
  --project=latam-data-challenge \
  --format="table(timestamp,severity,textPayload)" \
  --limit=50

# Filtrar por severidad
gcloud logging read "resource.type=cloud_run_job AND severity>=WARNING" \
  --project=latam-data-challenge
```

#### M√©tricas Disponibles

- **Tiempo de ejecuci√≥n** por m√©todo (time vs memory)
- **Uso de memoria** durante procesamiento
- **Resultados por pregunta** guardados en BigQuery
- **Logs estructurados** con timestamps y contexto

#### Consultas BigQuery

```sql
-- Comparar rendimiento entre m√©todos
SELECT 
  method,
  COUNT(*) as total_results,
  MAX(ingested_at) as last_execution
FROM `latam-data-challenge.challenge_data.q1_results`
GROUP BY method;

-- Ver evoluci√≥n temporal de resultados
SELECT 
  DATE(ingested_at) as execution_date,
  method,
  COUNT(*) as records_processed
FROM `latam-data-challenge.challenge_data.q2_results`
GROUP BY execution_date, method
ORDER BY execution_date DESC;
```

### Contribuci√≥n

1. Fork el proyecto
2. Crea una rama feature (`git checkout -b feature/nueva-feature`)
3. Commit tus cambios (`git commit -am 'Agrega nueva feature'`)
4. Push a la rama (`git push origin feature/nueva-feature`)
5. Abre un Pull Request

### Contacto

**Autor:** Fernando Landero  
**Email:** landerox@gmail.com  
**GitHub:** [@landerox](https://github.com/landerox)

---
